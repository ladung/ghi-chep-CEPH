### Object

Ceph Storage Cluster nhận dữ liệu từ Ceph Client ( các thành phần có thể truy cập vào Ceph Storage Cluster ) - có thể là Ceph Block Device, Ceph Object Storage, Ceph Filesystem, chính là các service interface được librados cung cấp, tất cả các service interface đều được Storage Cluster lưu trữ dưới dạng object.

Object gồm các thành phần data và metadata được đóng gói. Object được cung cấp 1 thuộc tính định danh ( globally unique identifier ) đảm bảo object sẽ độc nhất trong storage cluster. 

Mỗi object giống như một file trong filesystem, cả hai đều đóng vai trò trừu tượng để lưu trữ data nhưng chúng khác nhau về chức năng. File-based storage bị giới hạn bởi cấu trúc và ngữ nghĩa thực thi bởi filesystem cơ bản. Blocks trong filesystem thường bị fix size. Việc custom metadata cho một file truyền thống có thể bị hạn chế hoặc không thực hiện được. Trong Ceph, metadata của object có thể được custom và mỗi object có thể có size tùy ý, đặc điểm của nó không phụ thuộc vào các object khác trong cluster. 

<img src="https://github.com/VuVinh00/Images/blob/master/object.png">

Ceph không lưu trữ dạng dạng phân cấp như với filesystem có file và thư mục. Tất cả object được lưu trong flat namespace. Tránh việc phân cấp thứ bậc giúp Ceph có thể scale out ( mở rộng ) quy mô với hàng ngàn máy và thiết bị. Ceph lưu trữ Objects trong OSDs. Các object được nhóm lại với nhau và map tới OSDs bằng cách sử dụng thuật toán CRUSH. Object được gom lại thành một bộ lớn hơn được biết như là placement groups ( PG )

### Accessing object

Tất cả objects được phân bổ trong user-specified namespace được gọi là *pool*. Ceph pool là một logical partition của tất cả object trong phạm vi không gian lưu trữ của nó. Object có thể được replicated trong pool nhưng không thể tự động replicate thông qua nhiều pool.

Khi một Ceph cluster mới được triển khai, nó cần pool để lưu trữ data. Mặc định pool có thể được tạo ra tùy theo release của Ceph và loại dịch vụ được triển khai. 

## Placement groups

Ceph phân tán object ngẫu nhiên tới các thiết bị storage dựa trên data assignment policies đã được định trước. Assignment policy sử dụng một thuận toán được gọi là CRUSH để chọn probabilistically balanced distribution của object trên toàn cluster. Khi cluster được mở rộng bằng cách add thêm storage hay xóa bỏ các thiết bị storage, nó sẽ khôi phục lại sự cân bằng của cluster. 

Khi Ceph nhận được request lưu data, nó cần xác định vị trí đặt object. Mỗi object được map tới một logical collection của objects được gọi là PGs. Một object chỉ có thể map tới một PG tại một thời điểm. Sự mapping giữa object và PG sẽ tồn tại liên tục cho đến khi ta thay đổi số number của PGs trong pool. Matching PG sẽ được tính toán sử dụng thuật toán CRUSH hasing. Input để hashing là pool ID và object name. Sử dụng pool ID, nó sẽ lấy thông tin của pool và sẽ chỉ ra số lần replication hoặc chính sách mã hóa và có bao nhiêu bản sao của Object pool nên lưu trữ. Pool ID và object name giúp ta xác định **Placement Group ID (pgid)**

Giả sử ta thiết lập hệ số replication là 3, mỗi PG trong pool phải đảm bảo đối tượng trong healthy state, nó replicate chính xác 3 lần trên thiết bị hoặc OSDs khác nhau. PGs được thiết kế để thực thi nhất quán các bản copies của object

Nếu không có placement groups, nó sẽ khó cho việc quản trị, theo dõi các obj được nhân bản (hảng triệu obj được nhân bản) tới hàng trăm các OSD khác nhau. Thay vì quản lý tất cả obj riêng biệt, hệ thông cần quản lý placement group với numerous objects (số lượng nhiều các obj). Nó khiến ceph dễ quản lý và giảm bớt sự phức tạp

<img src="https://github.com/VuVinh00/Images/blob/master/PG.png">

Một đặc điểm quan trọng của PG là sự mapping giữa pgid và object name không được lưu trữ tại client hay OSDs hay MONs. Thực tế, sự mapping này không được lưu trữ tại bất kỳ đâu. Nó sẽ tính lại tự động và độc lập khi có data hoặc control request được gửi từ Ceph client. 

Objects trong mỗi pool được thu thập vào trong PGs và mỗi PGs sẽ phân phối tới các OSDs theo policy của pool. Mỗi PG lưu trữ hàng ngàn đến hàng triệu Object và mỗi OSD lưu trữ hàng chục đến hàng trăm PGs.

### PG Status
- **Active**: PG được đánh dấu active khi quá trình peering hoàn thành. Khi PG ở trong trạng thái active, client có thể thực hiện I/O
- **Clean** : Primary và secondary OSDs đã hoàn thành peering và tất cả Object được replicate đúng số lần.
- **Peering**: Tất cả các OSD đang thỏa thuận về trạng thái Object của chúng. Khi peering hoàn thành, OSDs sẽ đánh dấu PGs với trạng thái ``clean`` hoặc với trạng thái khác
- **Recovering**: Khi add thêm một OSD mới vào cluster hoặc khi có một OSD down, PG sẽ chuyển sang trạng thái recovering để đưa các bản replicate lên làm primary
- **Backfilling**: Ceph cần phải cân bằng lại dữ liệu khi một OSD mới được thêm vào cluster. Nó sẽ thực hiện hoạt động rebalancing bằng cách di chuyển một số PGs từ các OSDs khác tới OSDs mới. Hoạt động này được gọi là **backfill**. Khi rebalancing thành công khi OSD có thể thực hiện I/O

## CRUSH

Kỹ thuật lưu trữ truyền thống sử dụng để lưu trữ data và meta data. Metadata là dữ liệu về các data được lưu, mổ ta về dữ liệu, nơi data được lưu thực thế (series of storage nodes and disk arrays). Mỗi thời điểm data được thêm mới vào storage system, metadata sẽ được cập nhật thông tiên về ví trí ghi, sau đó là nơi data được lưu thực sự.

Phương sử dụng tốt khi lưu trữ nhỏ (vài TB -> vài trăm TB) nhưng khi dữ liệu lên tới EB, pp xuất hiện các hạn chế (tính chịu lỗi đơn thấp (mất metadata => mất toàn bộ dữ liệu)) => pp giải quyết tạo những bản sao, lưu tại các vị trí khác nhau tăng tính chịu lỗi => pp phức tạp, chứ nhiều vấn đề về storage system's scalability, high availability, and performance.

Ceph đem đến cuộc cách mạng về data storage và khả năng quản lý. Nó sử dụng tt Crush, phân phối dữ liệu thông minh qua Ceph. Thuật toán CRUSH là thành phần khiến Ceph trở nên nổi bật. Nó giúp Ceph tính toán quyết định vị trí lưu trữ. Thay vì lưu trữ metadata, CRUSH tính toán metadata khi yêu cầu, loại bỏ các điểm yếu khi sử dụng metadata theo cách truyền thống.

## Ceph pools

Ceph pool là một logical partition để lưu trữ object. Ceph Client nhận Cluster Map từ MON và write object vào pools. Với thông số "pool size" là số replicate, dựa vào CRUSH rule và placement groups sẽ xác định cách Ceph đặt dữ liệu như tế nào

<img src="https://github.com/VuVinh00/Images/blob/master/pool.png">

Trên mỗi pool sẽ chứa 3 thông số cơ bản : 

- Access permission
- Number of Placement Group
- Crush rule to use
