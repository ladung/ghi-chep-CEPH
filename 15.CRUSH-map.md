Thuật toán CRUSH quyết làm thế nào để lưu trữ và khôi phục data bằng cách tính toán data location

### CRUSH Location

Vị trí của OSD trong trong bản đồ phân cấp CRUSH được gọi là **crush location**. Vị trí này có dạng key và value để mô tả vị trí. Ví dụ nếu một OSD cụ thể nằm trong row, rack, chassis và host là một phần của "default" CRUSH tree, crush location sẽ được mô tả như sau:

``root=default row=a rack=a2 chassis=a2a host=a2a1``

Note:

- Thứ tự các key không quan trọng
- Key name (bên trái =) phải là một CRUSH type hợp lệ. Mặc định gồm :  root, datacenter, room, row, pod, pdu, rack, chassis và host
- Không cần xác định tất cả các key. Ví dụ, mặc định thì Ceph sẽ set **ceph-osd** location ở ``root=default host=HOSTNAME``

crush location cho OSD thường được thể hiện qua **crush location** option được set trong ``ceph.conf``. Mỗi khi OSD start, nó sẽ xác minh nó ở đúng vị trí trong CRUSH map, nếu không nó sẽ tự di chuyển chính nó. Để vô hiệu hóa việc tự động quản lý CRUSH map, thêm dòng sau vào config file trong phần [OSD]

``osd crush update on start = false``

### CRUSH STRUCTURE

CRUSH map gồm một hệ thống phân cấp mô tả topology vật lý của cluster và rules quy định policy về cách lưu trữ data trên thiết bị. 

#### Device

Device là ``ceph-osd`` daemon có thể lưu trữ data. Device được xác định bởi một id và tên thường là osd.N trong đó N là device id

Device cũng có ``device class`` được gắn liền với chúng (ví dụ: hdd hoặc ssd)

#### Types và buckets

Buckets là thuật ngữ cho các node trong bản đồ phân cấp như: hosts, racks, rows, ... CRUSH map định nghĩa các type được dùng để mô tả các node. Mặc định các type bao gồm: osd (hoặc device), host, chassis, rack, row, pdu, pod, room, datacenter, region, root,...

Mỗi node(device hoặc bucket) trong hệ thống phân cấp có chỉ số ``weight`` cho thấy tỷ lệ tương đối của tổng số dữ liệu mà thiết bị hoặc hệ thống phân cấp có thể lưu trữ

#### Rules

Rules quy định policy về cách dữ liệu được phân phối trên các devices trong hệ thống phân cấp

CRUSH rules xác định vị trí và và replication hoặc distribution policy cho phép ta xác định chính xác nơi CRUSH phân phối các bản sao.

Hầu hết ta có thể tạo CRUSH rules thông qua CLI bằng cách xác định ``pool type`` sẽ được sử dụng cho replicated hoặc erasure coded, ``failure domain`` và tùy chọn ``device class``. 

Ta có thể xem các rule được xác định cho cluster:

``ceph osd crush rule ls``

Ta có thể xem nội dung của rule:

``ceph osd crush rule dump``

#### Device classes

Mỗi device có một tùy chọn ``class`` gắn với nó. Mặc định, OSDs tự động thiết lập class của chúng khi khởi động có thể là: hdd, ssd hoặc nvme

Set device class cho một hoặc


### Modifying the crush map

#### Add/move OSD

Để add hoặc di chuyển OSD trong CRUSH map của một cluster đang chạy

``ceph osd crush set {name} {weight} root={root} [{bucket-type}={bucket-name} ...]``

Ví dụ add osd.0 vào hệ thống phân cấp hoặc di chuyển OSD

``ceph osd crush set osd.0 1.0 root=default datacenter=dc1 room=room1 row=foo rack=bar host=foo-bar-1``

#### Điều chỉnh OSD Weight

Để chiều chỉnh crush weight của OSD trong CRUSH map thực thi câu lệnh:

``ceph osd crush reweight {name} {weight}``

#### Remove OSD

``ceph osd crush remove {name}``

#### Add a bucket

Để add backet vào CRUSH map thực thi câu lệnh ``ceph osd crush add-bucket``

``ceph osd crush add-bucket {bucket-name} {bucket-type}``

#### Di chuyển bucket

``ceph osd crush move {bucket-name} {bucket-type}={bucket-name}, [...]``

#### Xóa bucket

``ceph osd crush remove {bucket-name}``

#### Create rule cho replicated pool

Với replicated pool, công việc chính khi tạo CRUSH rule là xác định failure domain. Ví dụ, nếu failure domain của host được chọn, sau đó CRUSH sẽ chắc chắn rằng mỗi bản repilca của data sẽ được lưu trữ tại các host khác nhau. Nếu rack được chọn, mỗi bản replica sẽ được lưu trữ trên rack khác nhau.

Nó cũng có thể tạo rule để giới hạn vị trí dữ liệu đến một ``class`` của device. Mặc định, Ceph OSDs tự động gán là hdd hay ssd tùy thuộc vào thiết bị đang sử dụng

Để tạo replicate rule:

``ceph osd crush rule create-replicated {name} {root} {failure-domain-type} [{class}]``

Thử tạo một replicate rule với failure-domain là OSD và chỉ cho lưu dữ liệu vào các ổ HDD. Sau đó tạo một pool với replicate rule vừa tạo và kiểm tra

Kiểm tra các ổ hdd, ssd :

```
[root@node3 ~]# ceph osd tree
ID  CLASS WEIGHT  TYPE NAME      STATUS REWEIGHT PRI-AFF
-13             0 root site2
 -1       0.20096 root default
 -3       0.06439     host node1
  0   hdd 0.04880         osd.0      up  1.00000 1.00000
  3   ssd 0.01559         osd.3      up  1.00000 1.00000
 -5       0.06828     host node2
  4   hdd 0.01949         osd.4      up  1.00000 1.00000
  1   ssd 0.04880         osd.1      up  1.00000 1.00000
 -7       0.06828     host node3
  2   hdd 0.04880         osd.2      up  1.00000 1.00000
  5   ssd 0.01949         osd.5      up  1.00000 1.00000
```

Kiểm tra các pg trên các OSD : ceph pg dump

```
OSD_STAT USED   AVAIL   USED_RAW TOTAL   HB_PEERS    PG_SUM PRIMARY_PG_SUM
5        15 MiB  19 GiB  1.0 GiB  20 GiB [0,1,2,3,4]      0              0
3        15 MiB  15 GiB  1.0 GiB  16 GiB [0,1,2,4,5]      0              0
0        15 MiB  49 GiB  1.0 GiB  50 GiB [1,2,3,4,5]      0              0
1        15 MiB  49 GiB  1.0 GiB  50 GiB [0,2,3,4,5]      0              0
4        15 MiB  19 GiB  1.0 GiB  20 GiB [0,1,2,3,5]      0              0
2        15 MiB  49 GiB  1.0 GiB  50 GiB [0,1,3,4,5]      0              0
sum      89 MiB 200 GiB  6.1 GiB 206 GiB
```

Tạo replicate rule có tên là rep-rule:

```
ceph osd crush rule create-replicated rep-rule default osd hdd
```

Tạo một pool và gắn rep-rule vào pool:

```
[root@node3 ~]# ceph osd pool create pool-rep-hdd 128 rep-rule
pool 'pool-rep-hdd' created
```

Kiểm tra lại các PG trên các OSD: ceph pg dump

```
OSD_STAT USED   AVAIL   USED_RAW TOTAL   HB_PEERS    PG_SUM PRIMARY_PG_SUM
5        15 MiB  19 GiB  1.0 GiB  20 GiB [0,1,2,3,4]      0              0
3        15 MiB  15 GiB  1.0 GiB  16 GiB [0,1,2,4,5]      0              0
0        15 MiB  49 GiB  1.0 GiB  50 GiB [1,2,3,4,5]    128             55
1        15 MiB  49 GiB  1.0 GiB  50 GiB [0,2,3,4,5]      0              0
4        15 MiB  19 GiB  1.0 GiB  20 GiB [0,1,2,3,5]    128             26
2        15 MiB  49 GiB  1.0 GiB  50 GiB [0,1,3,4,5]    128             47
```

Ta thấy số PG chỉ đc lưu trữ trên các OSD có device class là hdd và failure domain cũng được chia đều ra các OSD


#### Create rule erasure coded pool

Đối với erasure-coded pool, việc tạo pool cũng gần giống so với replicated pool. Erasure code pool được tạo có một chút khác biệt, tuy nhiên, bởi vì họ cần xây dựng cẩn thận dựa trên erasure code được sử dụng. Vì vậy, ta cần tông hợp thông tin erasure code vào trong ``erasure code profile``. CRUSH rule và sẽ được sử dụng khi tạo ra một pool

Erasure code profile có thể list với câu lệnh:

``ceph osd erasure-code-profile ls``

Xem các profile đang có sẵn:

``ceph osd erasure-code-profile get {profile-name}``

Một erasure code profile bao gồm các cặp key=value. Các tính chất đáng chú ý:

- **crush-root**: tên của CRUSH node để lưu trữ data (mặc định: default)
- **crush-failure-domain**: CRUSH type để lưu trữ các mảnh erasure-coded (mặc định: host)
- **crush-device-class**: device class để lưu trữ dữ liệu (default: none)
- **k** và **m**: xác định số lượng của mảnh erasure code, 

Khi profile được xác định, ta có thể tạo CRUSH rule:

``ceph osd crush rule create-erasure {name} {profile-name}``

#### Delete rules

``ceph osd crush rule rm {rule-name}``


